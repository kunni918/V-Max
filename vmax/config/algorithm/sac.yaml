defaults:
  - /network/base
  - _self_

name: SAC
unroll_length: 1
learning_rate: 1e-4
discount: 0.99
tau: 0.005
alpha: 0.2
batch_size: 64
grad_updates_per_step: 4
buffer_size: 1_000_000
learning_start: 50_000

network:
  policy:
    layer_sizes: [256, 64, 32]
    activation: relu  # gelu, relu, selu
    final_activation: null

  value:
    layer_sizes: [256, 64, 32]
    activation: relu  # gelu, relu, selu
    final_activation: relu  # tanh, relu
    num_networks: 2
    shared_encoder: false
